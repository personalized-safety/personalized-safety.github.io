<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Yuchen Wu</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Edward Sun</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Kaijie Zhu</a><sup>3</sup>,</span>
                    <span class="author-block">
                      <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jianxun Lian</a><sup>4</sup>,</span>
                      <span class="author-block">
                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jose Hernandez-Orallo</a><sup>5</sup>,</span>
                        <span class="author-block">
                          <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Aylin Caliskan</a><sup>1</sup>,</span>
                          <span class="author-block">
                            <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jindong Wang</a><sup>6</sup>,
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Washingtonr</span>
                    <span class="author-block"><sup>2</sup>University of California, Los Angeles</span>
                    <span class="author-block"><sup>3</sup>University of California, Santa Barbara</span>
                    <span class="author-block"><sup>4</sup>Microsoft Research Asia</span>
                    <span class="author-block"><sup>5</sup>Valencian Research Institute for Artificial Intelligence</span>
                    <span class="author-block"><sup>6</sup>William & Mary</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.arxiv.org/abs/2505.18882" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/wick1d/Personalized_Safety_Data" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" role="img" aria-label="hugging face">ðŸ¤—</span>
                      <span>Huggingface</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Chibadaisuki/Personalized-Safety-in-LLMs" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2505.18882" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Large language models (LLMs) typically generate identical or similar responses for all users given the same prompt, posing serious safety risks in high-stakes applications where user vulnerabilities differ widely.
Existing safety evaluations primarily rely on context-independent metricsâ€”such as factuality, bias, or toxicityâ€”overlooking the fact that the same response may carry divergent risks depending on the user's background or condition.
We introduce ``personalized safety'' to fill this gap and present PENGUINâ€”a benchmark comprising 14,000 scenarios across seven sensitive domains with both context-rich and context-free variants. Evaluating six leading LLMs, we demonstrate that personalized user information significantly improves safety scores by $43.2\%$, confirming the effectiveness of personalization in safety alignment. However, not all context attributes contribute equally to safety enhancement. To address this, we develop RAISEâ€”a training-free, two-stage agent framework that strategically acquires user-specific background. RAISE improves safety scores by up to 31.6% over six vanilla LLMs, while maintaining a low interaction cost of just 2.7 user queries on average. Our findings highlight the importance of selective information gathering in safety-critical domains and offer a practical solution for personalizing LLM responses without model retraining. This work establishes a foundation for safety research that adapts to individual user contexts rather than assuming a universal harm standard.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Motivation -->
<section class="section">
  
  <div class="container is-max-desktop">
    <div class="has-text-centered">
      <h2 class="title is-3">Personalized Safety Problem is a common issue</h2>
    </div>
      <figure class="image" style="max-width: 60%; margin: 2rem auto;">
      <img src="static/images/motivation.png" alt="Motivation" />
    </figure>
    <div class="content">
      <p>
Left (blue dashed box): Two users with different personal contexts ask the same sensitive query, but a generic response leads to divergent safety outcomesâ€”harmless for one, harmful for the other. Left (blue region): Evaluating this query across 1,000 diverse user profiles reveals highly inconsistent safety scores across models. Right (orange dashed box): When user-specific context is included, LLMs produce safer and more empathetic responses. Right (orange region): This trend generalizes across 14,000 context-rich scenarios, motivating our Penguin Benchmark for evaluating personalized safety in high-risk settings.      </p>
    </div>
  </div>
</section>

<section class="section">
      <div class="container is-max-desktop">
        <div class="has-text-centered">
      <h2 class="title is-3">PENGUIN BENCHMARK</h2>
          </div>
      <div class="content">
        <p>
          PENHGUIN benchmark (Personalized Evaluation of Nuanced Generation Under Individual Needs).
        </p>
        <figure class="image" style="margin: 1rem 0;">
          <img src="static/images/benchmark.png" alt="PENGUIN Benchmark">
        </figure>
        <div class="content">
        <p>
          The first large-scale testbed for evaluating LLM safety in personalized, high-stake scenarios. Each user scenario is associated with structured context attributes and is paired with both context-rich and context-free queries. These are scored on a three-dimensional personalized safety scale to quantify the impact of user context information.
      </div>
    </div>
  
  <div class="container is-max-desktop">
    <div class="box">
      <h3 class="subtitle is-4">Example Data</h3>
      <div class="content">
        <p>
          We collect data from both Reddit and Sythetic Data. Below is an example of Reddit Data.
        </p>
        <figure class="image" style="margin: 1rem 0;">
          <img src="static/images/Data.png" alt="Example Data">
        </figure>
      </div>
    </div>
    
    
    <div class="box">
      <h3 class="subtitle is-4">Experiment</h3>
      <div class="content">
        <p>
          Experiments among six state-of-art LLMs.
        </p>
        <figure class="image" style="margin: 1rem 0;">
          <img src="static/images/radar_subplots_combined.png" alt="Baseline">
        </figure>
        <p>
all models demonstrate substantial improvements with personalized context information.
On average, safety scores increase from 2.79 to 4.00 across the dataset, reflecting a consistent and significant trend.
These results indicate that the benefits of personalized information generalize across diverse model architectures and capability levels, which also motivates our next question: \textit{which user attributes contribute most to improving personalized safety?}
This question is particularly realistic given that collecting full context is not always feasible in real-world applications.        </p>
      </div>
    </div>
</section>

<!-- Method Section -->
<section class="section">
    <div class="container is-max-desktop">
      <div class="has-text-centered">
      <h2 class="title is-4">RAISE (Risk-Aware Information Selection Engine)</h2>
         </div>
      <div class="content">
        <figure class="image" style="max-width: 60%; margin: 2rem auto;">
          <img src="static/images/method.png" alt="RAISE method">
        </figure>
        <p>
Overview of our proposed RAISE framework. \em{Left}: We formulate the task as a sequential attribute selection problem, where each state represents a partial user context. \em{Middle}: An offline LLM-guided Monte Carlo Tree Search (MCTS) planner explores this space to discover optimized acquisition paths that maximize safety scores under budget constraints. \em{Right}: At inference time, the online agent follows the retrieved path via an Acquisition Module, while an Abstention Module decides when context suffices for safe response generation.        </p>
      </div>
    </div>
    
    <div class="container is-max-desktop">
      <h3 class="subtitle is-4">Performance Improvement by RAISE</h3>
      <div class="content">

       <figure class="image" style="max-width: 60%; margin: 2rem auto;">
          <img src="static/images/table.png" alt="Ablation Results">
        </figure>
        <p>
We denote the unmodified, context-free model as the vanilla baseline, which achieves an average safety score of only 2.86. Adding the abstention mechanism enables the model to determine when more user information is needed, avoiding unsafe responses when information is insufficient, thus improving safety scores to 3.56 (a 24.5% improvement). Further introducing the path planner allows the system to intelligently select the most valuable attribute query sequence, maximizing safety scores to 3.77 (an additional 5.9% improvement) while keeping query counts moderate (an average of just 2.7 queries per user (2.5 +Agent);

From the baseline model to the complete RAISE framework, safety scores improve by 31.6% overall. The planner helps optimize attribute collection strategies, while the abstention mechanism ensures generation is deferred until sufficient information is gathered. Together, they create a system that is both safe and efficient for high-stakes personalized LLM use cases.         </p>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{wu2025personalizedsafetyllmsbenchmark,
      title={Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach}, 
      author={Yuchen Wu and Edward Sun and Kaijie Zhu and Jianxun Lian and Jose Hernandez-Orallo and Aylin Caliskan and Jindong Wang},
      year={2025},
      eprint={2505.18882},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2505.18882}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
